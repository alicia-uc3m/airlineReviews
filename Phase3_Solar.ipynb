{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airlines Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 - LLM Solar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "Setting Up Hugging Face to Use the E: Drive instead of the default C: drive.\n",
    "This saves local disk space and helps manage large files better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets peft accelerate bitsandbytes \n",
    "import os\n",
    "\n",
    "# Store all Hugging Face files on the E: drive\n",
    "os.environ[\"HF_HOME\"] = \"E:/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"E:/huggingface/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"E:/huggingface/datasets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1660 Ti\n",
      "Total VRAM: 6.44 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(f\"Total VRAM: {round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "The model is loaded and prepared for efficient fine-tuning:\n",
    "\n",
    "- First, 4-bit quantization is set up using BitsAndBytesConfig to save GPU memory.\n",
    "\n",
    "- The tokenizer and base model are loaded from HuggingFace, configured for a 3-class classification task.\n",
    "\n",
    "- The model is then adapted for 4-bit training with prepare_model_for_kbit_training.\n",
    "\n",
    "- Finally, LoRA (Low-Rank Adaptation) is applied by injecting lightweight trainable layers into the attention mechanisms (q_proj and v_proj), making the fine-tuning process much faster and lighter.\n",
    "\n",
    "- The padding token is also corrected after these adjustments to ensure input sequences are properly handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at upstage/TinySolar-248m-4k-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: </s>\n",
      "Pad token ID: 2\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Setup 4-bit Quantization : configure the model to load in 4-bit precision to save memory (important with small VRAM GPUs like 6 GB).\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "#  Load Tokenizer\n",
    "model_id = \"upstage/TinySolar-248m-4k-py\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  #  Add padding\n",
    "\n",
    "# Base model: Load Pre-trained\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    num_labels=3, # ready for classification tasks with 3 output classes\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "# ‚úÖ Prepare for LoRA after quant: adapt the model for training in 4-bit precision, making it faster and lighter to fine-tune.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA Fine-Tuning : configure LoRA (Low-Rank Adaptation) to inject small, efficient trainable adapters into the model‚Äôs attention layers.\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Now apply pad_token_id after PEFT\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Pad token:\", tokenizer.pad_token)\n",
    "print(\"Pad token ID:\", tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Verified</th>\n",
       "      <th>Type of Traveller</th>\n",
       "      <th>Month Flown</th>\n",
       "      <th>Route</th>\n",
       "      <th>Class</th>\n",
       "      <th>Seat Comfort</th>\n",
       "      <th>Staff Service</th>\n",
       "      <th>Food &amp; Beverages</th>\n",
       "      <th>Inflight Entertainment</th>\n",
       "      <th>Value For Money</th>\n",
       "      <th>Overall Rating</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Review_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alison Soetantyo</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>True</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>December 2023</td>\n",
       "      <td>Jakarta to Singapore</td>\n",
       "      <td>Business Class</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>yes</td>\n",
       "      <td>Flight was amazing.   Flight was amazing. The ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert Watson</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>True</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>February 2024</td>\n",
       "      <td>Phuket to Singapore</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>seats on this aircraft are dreadful . ¬†¬†Bookin...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S Han</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>True</td>\n",
       "      <td>Family Leisure</td>\n",
       "      <td>February 2024</td>\n",
       "      <td>Siem Reap to Singapore</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>yes</td>\n",
       "      <td>Food was plentiful and tasty.   Excellent perf...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D Laynes</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>True</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>February 2024</td>\n",
       "      <td>Singapore to London Heathrow</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>yes</td>\n",
       "      <td>‚Äúhow much food was available.  Pretty comforta...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Othman</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>True</td>\n",
       "      <td>Family Leisure</td>\n",
       "      <td>February 2024</td>\n",
       "      <td>Singapore to Phnom Penh</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>yes</td>\n",
       "      <td>‚Äúservice was consistently good‚Äù.  The service ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name Review Date             Airline Verified  \\\n",
       "0  Alison Soetantyo  2024-03-01  Singapore Airlines     True   \n",
       "1     Robert Watson  2024-02-21  Singapore Airlines     True   \n",
       "2             S Han  2024-02-20  Singapore Airlines     True   \n",
       "3          D Laynes  2024-02-19  Singapore Airlines     True   \n",
       "4         A Othman   2024-02-19  Singapore Airlines     True   \n",
       "\n",
       "  Type of Traveller    Month Flown                         Route  \\\n",
       "0      Solo Leisure  December 2023          Jakarta to Singapore   \n",
       "1      Solo Leisure  February 2024           Phuket to Singapore   \n",
       "2    Family Leisure  February 2024        Siem Reap to Singapore   \n",
       "3      Solo Leisure  February 2024  Singapore to London Heathrow   \n",
       "4    Family Leisure  February 2024       Singapore to Phnom Penh   \n",
       "\n",
       "            Class  Seat Comfort  Staff Service  Food & Beverages  \\\n",
       "0  Business Class             4              4                 4   \n",
       "1   Economy Class             5              3                 4   \n",
       "2   Economy Class             1              5                 2   \n",
       "3   Economy Class             5              5                 5   \n",
       "4   Economy Class             5              5                 5   \n",
       "\n",
       "   Inflight Entertainment  Value For Money  Overall Rating Recommended  \\\n",
       "0                       4                4               9         yes   \n",
       "1                       4                1               3          no   \n",
       "2                       1                5              10         yes   \n",
       "3                       5                5              10         yes   \n",
       "4                       5                5              10         yes   \n",
       "\n",
       "                                              Review Sentiment  Review_Length  \n",
       "0  Flight was amazing.   Flight was amazing. The ...  Positive             89  \n",
       "1  seats on this aircraft are dreadful . ¬†¬†Bookin...  Negative             49  \n",
       "2  Food was plentiful and tasty.   Excellent perf...  Positive             34  \n",
       "3  ‚Äúhow much food was available.  Pretty comforta...  Positive            171  \n",
       "4  ‚Äúservice was consistently good‚Äù.  The service ...  Positive             57  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./AirlinesReviews/data_phase1.csv\", encoding='utf-8', on_bad_lines='skip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "The dataset is cleaned and prepared by mapping sentiment labels (Positive, Negative, Neutral) to numerical values required for model training. \n",
    "\n",
    "Only the review text and corresponding label were retained, with the text column renamed to text for compatibility with the tokenizer. \n",
    "\n",
    "The \"Review\" column is renamed to \"text\" to match HuggingFace standards.\n",
    "\n",
    "The dataset is then split into training, validation, and test sets following an 80%-10%-10% split, ensuring random and reproducible partitions for fine-tuning the Solar model.\n",
    "\n",
    "This simplified structure ensures the data is ready for tokenization and fine-tuning ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# üí¨ Map sentiment labels to integers\n",
    "label_map = {'Negative': 0, 'Positive': 1, 'Neutral': 2}\n",
    "df = df[df['Sentiment'].isin(label_map.keys())]\n",
    "df['label'] = df['Sentiment'].map(label_map)\n",
    "df = df[['Review', 'label']].rename(columns={'Review': 'text'})\n",
    "\n",
    "# === STEP 1: Split original imbalanced data FIRST ===\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    stratify=df['label'],  # maintain class ratio in test\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === STEP 2: Upsample Neutral class ONLY in training set ===\n",
    "df_neg = train_df[train_df['label'] == 0]\n",
    "df_pos = train_df[train_df['label'] == 1]\n",
    "df_neu = train_df[train_df['label'] == 2]\n",
    "\n",
    "# Match to largest class\n",
    "target_size = max(len(df_neg), len(df_pos))\n",
    "df_neu_upsampled = resample(\n",
    "    df_neu,\n",
    "    replace=True,\n",
    "    n_samples=target_size,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine balanced training set\n",
    "balanced_train_df = pd.concat([df_neg, df_pos, df_neu_upsampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# === STEP 3: Convert to Hugging Face datasets ===\n",
    "train_dataset = Dataset.from_pandas(balanced_train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# === STEP 4: Split train into train/validation (10% val) ===\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.1111, seed=42)\n",
    "\n",
    "# === STEP 5: Wrap in DatasetDict ===\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_val_split[\"train\"],\n",
    "    \"validation\": train_val_split[\"test\"],\n",
    "    \"test\": test_dataset  # ‚úÖ original imbalanced test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Dataset\n",
    "\n",
    "The dataset is tokenized using the same tokenizer as the Solar model:\n",
    "\n",
    "A custom tokenize function is applied to the \"text\" column, ensuring all sequences are padded or truncated to a maximum length of 512 tokens.\n",
    "The dataset is processed in batches for faster tokenization, preparing the text inputs for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d489b09e9f4e669a5c75e67f134137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7866 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cda423dba24d6e8b3ec1aedcb2cd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a20287295946b3a9a6ee9db77d8b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Search with Optuna\n",
    "\n",
    "Hyperparameter optimization is performed using Optuna:\n",
    "\n",
    "- The model is trained multiple times on the training set, and evaluated on the validation set, with the goal of maximizing the weighted F1-score.\n",
    "\n",
    "- Optuna explores different learning rates, batch sizes, and numbers of epochs to automatically find the best combination.\n",
    "\n",
    "- Training is monitored with a custom callback to track progress, and early stopping is used to avoid overfitting.\n",
    "\n",
    "- To address class imbalance in the dataset, class weights are computed automatically based on the frequency of each emotion label (Positive, Negative, Neutral).\n",
    "These weights are applied during training using a custom WeightedTrainer that modifies the loss function.\n",
    "This ensures that the model gives more importance to underrepresented classes and does not bias predictions toward majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alicia\\AppData\\Roaming\\Python\\Python39\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alicia\\AppData\\Local\\Temp\\ipykernel_20380\\1254717194.py:105: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at upstage/TinySolar-248m-4k-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[I 2025-05-28 15:17:28,596] A new study created in memory with name: no-name-26a45902-b598-4a13-bb73-74cf83c56a09\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at upstage/TinySolar-248m-4k-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting trial at 15:17:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3934' max='3934' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3934/3934 1:21:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638300</td>\n",
       "      <td>0.558187</td>\n",
       "      <td>0.804724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.511500</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.842322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Step 100 - Elapsed: 2.11 min - Logs: {'loss': 1.1037, 'grad_norm': 6.953557014465332, 'learning_rate': 0.00015071224052851426, 'epoch': 0.05083884087442806}\n",
      "‚è±Ô∏è Step 200 - Elapsed: 4.13 min - Logs: {'loss': 0.7301, 'grad_norm': 17.495779037475586, 'learning_rate': 0.00014678232552125183, 'epoch': 0.10167768174885612}\n",
      "‚è±Ô∏è Step 300 - Elapsed: 6.13 min - Logs: {'loss': 0.7663, 'grad_norm': 10.237431526184082, 'learning_rate': 0.0001428524105139894, 'epoch': 0.1525165226232842}\n",
      "‚è±Ô∏è Step 400 - Elapsed: 8.13 min - Logs: {'loss': 0.7545, 'grad_norm': 20.845762252807617, 'learning_rate': 0.00013892249550672697, 'epoch': 0.20335536349771224}\n",
      "‚è±Ô∏è Step 500 - Elapsed: 10.15 min - Logs: {'loss': 0.6788, 'grad_norm': 18.751005172729492, 'learning_rate': 0.00013499258049946454, 'epoch': 0.2541942043721403}\n",
      "‚è±Ô∏è Step 600 - Elapsed: 12.18 min - Logs: {'loss': 0.5493, 'grad_norm': 9.451436996459961, 'learning_rate': 0.0001310626654922021, 'epoch': 0.3050330452465684}\n",
      "‚è±Ô∏è Step 700 - Elapsed: 14.2 min - Logs: {'loss': 0.6915, 'grad_norm': 21.36066246032715, 'learning_rate': 0.00012713275048493967, 'epoch': 0.35587188612099646}\n",
      "‚è±Ô∏è Step 800 - Elapsed: 16.19 min - Logs: {'loss': 0.6513, 'grad_norm': 8.237410545349121, 'learning_rate': 0.00012320283547767724, 'epoch': 0.4067107269954245}\n",
      "‚è±Ô∏è Step 900 - Elapsed: 18.23 min - Logs: {'loss': 0.6257, 'grad_norm': 21.211362838745117, 'learning_rate': 0.00011927292047041482, 'epoch': 0.45754956786985257}\n",
      "‚è±Ô∏è Step 1000 - Elapsed: 20.26 min - Logs: {'loss': 0.5674, 'grad_norm': 46.08968734741211, 'learning_rate': 0.00011534300546315239, 'epoch': 0.5083884087442806}\n",
      "‚è±Ô∏è Step 1100 - Elapsed: 22.29 min - Logs: {'loss': 0.615, 'grad_norm': 18.836915969848633, 'learning_rate': 0.00011141309045588994, 'epoch': 0.5592272496187087}\n",
      "‚è±Ô∏è Step 1200 - Elapsed: 24.32 min - Logs: {'loss': 0.59, 'grad_norm': 12.17960262298584, 'learning_rate': 0.00010748317544862751, 'epoch': 0.6100660904931368}\n",
      "‚è±Ô∏è Step 1300 - Elapsed: 26.35 min - Logs: {'loss': 0.6215, 'grad_norm': 33.166622161865234, 'learning_rate': 0.00010355326044136508, 'epoch': 0.6609049313675648}\n",
      "‚è±Ô∏è Step 1400 - Elapsed: 28.45 min - Logs: {'loss': 0.6257, 'grad_norm': 32.164466857910156, 'learning_rate': 9.962334543410265e-05, 'epoch': 0.7117437722419929}\n",
      "‚è±Ô∏è Step 1500 - Elapsed: 30.48 min - Logs: {'loss': 0.632, 'grad_norm': 35.90655517578125, 'learning_rate': 9.569343042684023e-05, 'epoch': 0.762582613116421}\n",
      "‚è±Ô∏è Step 1600 - Elapsed: 32.48 min - Logs: {'loss': 0.5915, 'grad_norm': 33.8934326171875, 'learning_rate': 9.17635154195778e-05, 'epoch': 0.813421453990849}\n",
      "‚è±Ô∏è Step 1700 - Elapsed: 34.51 min - Logs: {'loss': 0.7273, 'grad_norm': 30.79454803466797, 'learning_rate': 8.783360041231535e-05, 'epoch': 0.8642602948652771}\n",
      "‚è±Ô∏è Step 1800 - Elapsed: 36.54 min - Logs: {'loss': 0.5693, 'grad_norm': 14.132806777954102, 'learning_rate': 8.390368540505292e-05, 'epoch': 0.9150991357397051}\n",
      "‚è±Ô∏è Step 1900 - Elapsed: 38.57 min - Logs: {'loss': 0.6383, 'grad_norm': 30.32113265991211, 'learning_rate': 7.997377039779049e-05, 'epoch': 0.9659379766141332}\n",
      "‚è±Ô∏è Step 1967 - Elapsed: 40.95 min - Logs: {'eval_loss': 0.5581871867179871, 'eval_f1': 0.8047243801735953, 'eval_runtime': 62.6719, 'eval_samples_per_second': 15.701, 'eval_steps_per_second': 1.963, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Step 2000 - Elapsed: 41.63 min - Logs: {'loss': 0.5797, 'grad_norm': 12.97858715057373, 'learning_rate': 7.604385539052805e-05, 'epoch': 1.0167768174885612}\n",
      "‚è±Ô∏è Step 2100 - Elapsed: 43.75 min - Logs: {'loss': 0.4638, 'grad_norm': 20.888425827026367, 'learning_rate': 7.211394038326562e-05, 'epoch': 1.0676156583629894}\n",
      "‚è±Ô∏è Step 2200 - Elapsed: 45.78 min - Logs: {'loss': 0.4231, 'grad_norm': 7.909877777099609, 'learning_rate': 6.818402537600319e-05, 'epoch': 1.1184544992374175}\n",
      "‚è±Ô∏è Step 2300 - Elapsed: 47.78 min - Logs: {'loss': 0.5937, 'grad_norm': 0.5770264863967896, 'learning_rate': 6.425411036874077e-05, 'epoch': 1.1692933401118455}\n",
      "‚è±Ô∏è Step 2400 - Elapsed: 49.78 min - Logs: {'loss': 0.5829, 'grad_norm': 34.508792877197266, 'learning_rate': 6.0324195361478324e-05, 'epoch': 1.2201321809862735}\n",
      "‚è±Ô∏è Step 2500 - Elapsed: 51.77 min - Logs: {'loss': 0.5011, 'grad_norm': 24.00214958190918, 'learning_rate': 5.63942803542159e-05, 'epoch': 1.2709710218607015}\n",
      "‚è±Ô∏è Step 2600 - Elapsed: 53.81 min - Logs: {'loss': 0.5832, 'grad_norm': 23.18834686279297, 'learning_rate': 5.246436534695347e-05, 'epoch': 1.3218098627351296}\n",
      "‚è±Ô∏è Step 2700 - Elapsed: 55.81 min - Logs: {'loss': 0.4585, 'grad_norm': 5.391473770141602, 'learning_rate': 4.853445033969103e-05, 'epoch': 1.3726487036095576}\n",
      "‚è±Ô∏è Step 2800 - Elapsed: 57.81 min - Logs: {'loss': 0.4736, 'grad_norm': 23.08452606201172, 'learning_rate': 4.46045353324286e-05, 'epoch': 1.4234875444839858}\n",
      "‚è±Ô∏è Step 2900 - Elapsed: 59.85 min - Logs: {'loss': 0.4233, 'grad_norm': 9.614174842834473, 'learning_rate': 4.067462032516617e-05, 'epoch': 1.4743263853584139}\n",
      "‚è±Ô∏è Step 3000 - Elapsed: 61.84 min - Logs: {'loss': 0.473, 'grad_norm': 23.70981216430664, 'learning_rate': 3.674470531790374e-05, 'epoch': 1.525165226232842}\n",
      "‚è±Ô∏è Step 3100 - Elapsed: 63.87 min - Logs: {'loss': 0.4274, 'grad_norm': 25.862852096557617, 'learning_rate': 3.2814790310641306e-05, 'epoch': 1.57600406710727}\n",
      "‚è±Ô∏è Step 3200 - Elapsed: 65.9 min - Logs: {'loss': 0.625, 'grad_norm': 21.77927017211914, 'learning_rate': 2.8884875303378877e-05, 'epoch': 1.6268429079816982}\n",
      "‚è±Ô∏è Step 3300 - Elapsed: 67.94 min - Logs: {'loss': 0.5515, 'grad_norm': 24.404022216796875, 'learning_rate': 2.495496029611644e-05, 'epoch': 1.6776817488561262}\n",
      "‚è±Ô∏è Step 3400 - Elapsed: 69.99 min - Logs: {'loss': 0.4487, 'grad_norm': 2.1584527492523193, 'learning_rate': 2.102504528885401e-05, 'epoch': 1.7285205897305542}\n",
      "‚è±Ô∏è Step 3500 - Elapsed: 71.99 min - Logs: {'loss': 0.5021, 'grad_norm': 38.96363067626953, 'learning_rate': 1.7095130281591577e-05, 'epoch': 1.7793594306049823}\n",
      "‚è±Ô∏è Step 3600 - Elapsed: 73.96 min - Logs: {'loss': 0.469, 'grad_norm': 2.3633267879486084, 'learning_rate': 1.3165215274329147e-05, 'epoch': 1.8301982714794103}\n",
      "‚è±Ô∏è Step 3700 - Elapsed: 75.96 min - Logs: {'loss': 0.4147, 'grad_norm': 21.27886199951172, 'learning_rate': 9.235300267066715e-06, 'epoch': 1.8810371123538383}\n",
      "‚è±Ô∏è Step 3800 - Elapsed: 77.97 min - Logs: {'loss': 0.4916, 'grad_norm': 3.643968105316162, 'learning_rate': 5.305385259804283e-06, 'epoch': 1.9318759532282663}\n",
      "‚è±Ô∏è Step 3900 - Elapsed: 79.97 min - Logs: {'loss': 0.5115, 'grad_norm': 16.190670013427734, 'learning_rate': 1.3754702525418513e-06, 'epoch': 1.9827147941026944}\n",
      "‚è±Ô∏è Step 3934 - Elapsed: 81.68 min - Logs: {'eval_loss': 0.5935133099555969, 'eval_f1': 0.8423215003377372, 'eval_runtime': 61.7973, 'eval_samples_per_second': 15.923, 'eval_steps_per_second': 1.99, 'epoch': 2.0}\n",
      "‚è±Ô∏è Step 3934 - Elapsed: 81.69 min - Logs: {'train_runtime': 4901.3885, 'train_samples_per_second': 3.21, 'train_steps_per_second': 0.803, 'total_flos': 8834654115201024.0, 'train_loss': 0.5809466735808814, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 16:39:11,485] Trial 0 finished with value: 0.8423215003377372 and parameters: {'learning_rate': 0.00015460285638570407, 'per_device_train_batch_size': 4, 'num_train_epochs': 2}. Best is trial 0 with value: 0.8423215003377372.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at upstage/TinySolar-248m-4k-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting trial at 16:39:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7866' max='7866' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7866/7866 1:10:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.120900</td>\n",
       "      <td>0.734232</td>\n",
       "      <td>0.815055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.762103</td>\n",
       "      <td>0.852110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Step 100 - Elapsed: 0.89 min - Logs: {'loss': 1.2068, 'grad_norm': 18.513290405273438, 'learning_rate': 0.00019223271127945376, 'epoch': 0.025425883549453344}\n",
      "‚è±Ô∏è Step 200 - Elapsed: 1.76 min - Logs: {'loss': 0.9667, 'grad_norm': 19.24113655090332, 'learning_rate': 0.00018975771821547213, 'epoch': 0.05085176709890669}\n",
      "‚è±Ô∏è Step 300 - Elapsed: 2.67 min - Logs: {'loss': 0.7846, 'grad_norm': 0.1570926010608673, 'learning_rate': 0.00018728272515149052, 'epoch': 0.07627765064836003}\n",
      "‚è±Ô∏è Step 400 - Elapsed: 3.55 min - Logs: {'loss': 0.8363, 'grad_norm': 3.243751049041748, 'learning_rate': 0.00018480773208750885, 'epoch': 0.10170353419781338}\n",
      "‚è±Ô∏è Step 500 - Elapsed: 4.45 min - Logs: {'loss': 1.0555, 'grad_norm': 56.86756896972656, 'learning_rate': 0.00018233273902352722, 'epoch': 0.12712941774726672}\n",
      "‚è±Ô∏è Step 600 - Elapsed: 5.28 min - Logs: {'loss': 0.9867, 'grad_norm': 6.906810283660889, 'learning_rate': 0.0001798577459595456, 'epoch': 0.15255530129672007}\n",
      "‚è±Ô∏è Step 700 - Elapsed: 6.1 min - Logs: {'loss': 0.8904, 'grad_norm': 18.907325744628906, 'learning_rate': 0.00017738275289556394, 'epoch': 0.1779811848461734}\n",
      "‚è±Ô∏è Step 800 - Elapsed: 6.92 min - Logs: {'loss': 1.2135, 'grad_norm': 1.712239384651184, 'learning_rate': 0.0001749077598315823, 'epoch': 0.20340706839562675}\n",
      "‚è±Ô∏è Step 900 - Elapsed: 7.75 min - Logs: {'loss': 0.8539, 'grad_norm': 19.53546714782715, 'learning_rate': 0.0001724327667676007, 'epoch': 0.2288329519450801}\n",
      "‚è±Ô∏è Step 1000 - Elapsed: 8.6 min - Logs: {'loss': 0.9563, 'grad_norm': 1.7683768272399902, 'learning_rate': 0.00016995777370361903, 'epoch': 0.25425883549453343}\n",
      "‚è±Ô∏è Step 1100 - Elapsed: 9.47 min - Logs: {'loss': 0.8147, 'grad_norm': 44.40712356567383, 'learning_rate': 0.0001674827806396374, 'epoch': 0.2796847190439868}\n",
      "‚è±Ô∏è Step 1200 - Elapsed: 10.33 min - Logs: {'loss': 0.853, 'grad_norm': 1.3594858646392822, 'learning_rate': 0.00016500778757565576, 'epoch': 0.30511060259344014}\n",
      "‚è±Ô∏è Step 1300 - Elapsed: 11.2 min - Logs: {'loss': 1.1494, 'grad_norm': 52.262935638427734, 'learning_rate': 0.00016253279451167412, 'epoch': 0.3305364861428935}\n",
      "‚è±Ô∏è Step 1400 - Elapsed: 12.06 min - Logs: {'loss': 0.9062, 'grad_norm': 6.9789347648620605, 'learning_rate': 0.0001600578014476925, 'epoch': 0.3559623696923468}\n",
      "‚è±Ô∏è Step 1500 - Elapsed: 12.92 min - Logs: {'loss': 0.9771, 'grad_norm': 43.82130813598633, 'learning_rate': 0.00015758280838371085, 'epoch': 0.38138825324180015}\n",
      "‚è±Ô∏è Step 1600 - Elapsed: 13.8 min - Logs: {'loss': 0.934, 'grad_norm': 0.11780504882335663, 'learning_rate': 0.00015510781531972922, 'epoch': 0.4068141367912535}\n",
      "‚è±Ô∏è Step 1700 - Elapsed: 14.75 min - Logs: {'loss': 0.8406, 'grad_norm': 0.4022670090198517, 'learning_rate': 0.00015263282225574758, 'epoch': 0.43224002034070685}\n",
      "‚è±Ô∏è Step 1800 - Elapsed: 15.63 min - Logs: {'loss': 0.857, 'grad_norm': 3.6324288845062256, 'learning_rate': 0.00015015782919176594, 'epoch': 0.4576659038901602}\n",
      "‚è±Ô∏è Step 1900 - Elapsed: 16.5 min - Logs: {'loss': 0.8458, 'grad_norm': 4.1267266273498535, 'learning_rate': 0.0001476828361277843, 'epoch': 0.4830917874396135}\n",
      "‚è±Ô∏è Step 2000 - Elapsed: 17.38 min - Logs: {'loss': 0.7845, 'grad_norm': 74.32234191894531, 'learning_rate': 0.00014520784306380267, 'epoch': 0.5085176709890669}\n",
      "‚è±Ô∏è Step 2100 - Elapsed: 18.23 min - Logs: {'loss': 0.925, 'grad_norm': 2.500404119491577, 'learning_rate': 0.00014273284999982103, 'epoch': 0.5339435545385202}\n",
      "‚è±Ô∏è Step 2200 - Elapsed: 19.08 min - Logs: {'loss': 0.8243, 'grad_norm': 0.01009322702884674, 'learning_rate': 0.0001402578569358394, 'epoch': 0.5593694380879736}\n",
      "‚è±Ô∏è Step 2300 - Elapsed: 19.95 min - Logs: {'loss': 1.0804, 'grad_norm': 112.33197021484375, 'learning_rate': 0.00013778286387185776, 'epoch': 0.5847953216374269}\n",
      "‚è±Ô∏è Step 2400 - Elapsed: 20.79 min - Logs: {'loss': 0.7455, 'grad_norm': 13.960872650146484, 'learning_rate': 0.00013530787080787612, 'epoch': 0.6102212051868803}\n",
      "‚è±Ô∏è Step 2500 - Elapsed: 21.65 min - Logs: {'loss': 0.9871, 'grad_norm': 0.43034783005714417, 'learning_rate': 0.00013283287774389449, 'epoch': 0.6356470887363336}\n",
      "‚è±Ô∏è Step 2600 - Elapsed: 22.5 min - Logs: {'loss': 0.8407, 'grad_norm': 86.62208557128906, 'learning_rate': 0.00013035788467991285, 'epoch': 0.661072972285787}\n",
      "‚è±Ô∏è Step 2700 - Elapsed: 23.34 min - Logs: {'loss': 0.8415, 'grad_norm': 3.8615753650665283, 'learning_rate': 0.0001278828916159312, 'epoch': 0.6864988558352403}\n",
      "‚è±Ô∏è Step 2800 - Elapsed: 24.18 min - Logs: {'loss': 1.1079, 'grad_norm': 32.734901428222656, 'learning_rate': 0.00012540789855194958, 'epoch': 0.7119247393846936}\n",
      "‚è±Ô∏è Step 2900 - Elapsed: 25.05 min - Logs: {'loss': 0.8523, 'grad_norm': 1.0948164463043213, 'learning_rate': 0.0001229329054879679, 'epoch': 0.737350622934147}\n",
      "‚è±Ô∏è Step 3000 - Elapsed: 25.92 min - Logs: {'loss': 0.8472, 'grad_norm': 84.19825744628906, 'learning_rate': 0.0001204579124239863, 'epoch': 0.7627765064836003}\n",
      "‚è±Ô∏è Step 3100 - Elapsed: 26.79 min - Logs: {'loss': 0.6485, 'grad_norm': 0.037795353680849075, 'learning_rate': 0.00011798291936000465, 'epoch': 0.7882023900330537}\n",
      "‚è±Ô∏è Step 3200 - Elapsed: 27.67 min - Logs: {'loss': 0.9425, 'grad_norm': 70.20957946777344, 'learning_rate': 0.00011550792629602302, 'epoch': 0.813628273582507}\n",
      "‚è±Ô∏è Step 3300 - Elapsed: 28.52 min - Logs: {'loss': 1.1792, 'grad_norm': 71.04743957519531, 'learning_rate': 0.00011303293323204139, 'epoch': 0.8390541571319603}\n",
      "‚è±Ô∏è Step 3400 - Elapsed: 29.37 min - Logs: {'loss': 1.0649, 'grad_norm': 65.90628814697266, 'learning_rate': 0.00011055794016805974, 'epoch': 0.8644800406814137}\n",
      "‚è±Ô∏è Step 3500 - Elapsed: 30.24 min - Logs: {'loss': 0.5729, 'grad_norm': 26.42604637145996, 'learning_rate': 0.0001080829471040781, 'epoch': 0.889905924230867}\n",
      "‚è±Ô∏è Step 3600 - Elapsed: 31.1 min - Logs: {'loss': 0.9663, 'grad_norm': 81.118896484375, 'learning_rate': 0.00010560795404009648, 'epoch': 0.9153318077803204}\n",
      "‚è±Ô∏è Step 3700 - Elapsed: 31.95 min - Logs: {'loss': 0.8804, 'grad_norm': 11.326884269714355, 'learning_rate': 0.00010313296097611483, 'epoch': 0.9407576913297737}\n",
      "‚è±Ô∏è Step 3800 - Elapsed: 32.8 min - Logs: {'loss': 1.1002, 'grad_norm': 65.02507019042969, 'learning_rate': 0.0001006579679121332, 'epoch': 0.966183574879227}\n",
      "‚è±Ô∏è Step 3900 - Elapsed: 33.66 min - Logs: {'loss': 1.1209, 'grad_norm': 45.44739532470703, 'learning_rate': 9.818297484815155e-05, 'epoch': 0.9916094584286804}\n",
      "‚è±Ô∏è Step 3933 - Elapsed: 34.91 min - Logs: {'eval_loss': 0.734232485294342, 'eval_f1': 0.8150550472126458, 'eval_runtime': 58.2025, 'eval_samples_per_second': 16.906, 'eval_steps_per_second': 2.113, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Step 4000 - Elapsed: 35.51 min - Logs: {'loss': 0.6299, 'grad_norm': 0.006468088831752539, 'learning_rate': 9.570798178416992e-05, 'epoch': 1.0170353419781337}\n",
      "‚è±Ô∏è Step 4100 - Elapsed: 36.41 min - Logs: {'loss': 0.5825, 'grad_norm': 0.00860884040594101, 'learning_rate': 9.323298872018827e-05, 'epoch': 1.0424612255275871}\n",
      "‚è±Ô∏è Step 4200 - Elapsed: 37.28 min - Logs: {'loss': 0.7244, 'grad_norm': 7.224157333374023, 'learning_rate': 9.075799565620665e-05, 'epoch': 1.0678871090770405}\n",
      "‚è±Ô∏è Step 4300 - Elapsed: 38.18 min - Logs: {'loss': 0.7432, 'grad_norm': 1.680328369140625, 'learning_rate': 8.8283002592225e-05, 'epoch': 1.0933129926264937}\n",
      "‚è±Ô∏è Step 4400 - Elapsed: 39.24 min - Logs: {'loss': 0.4468, 'grad_norm': 2.028311014175415, 'learning_rate': 8.580800952824336e-05, 'epoch': 1.1187388761759471}\n",
      "‚è±Ô∏è Step 4500 - Elapsed: 40.31 min - Logs: {'loss': 0.9334, 'grad_norm': 72.4071044921875, 'learning_rate': 8.333301646426173e-05, 'epoch': 1.1441647597254005}\n",
      "‚è±Ô∏è Step 4600 - Elapsed: 41.38 min - Logs: {'loss': 0.6734, 'grad_norm': 0.8571040630340576, 'learning_rate': 8.085802340028009e-05, 'epoch': 1.1695906432748537}\n",
      "‚è±Ô∏è Step 4700 - Elapsed: 42.3 min - Logs: {'loss': 0.8256, 'grad_norm': 0.092881940305233, 'learning_rate': 7.838303033629845e-05, 'epoch': 1.1950165268243071}\n",
      "‚è±Ô∏è Step 4800 - Elapsed: 43.19 min - Logs: {'loss': 0.7066, 'grad_norm': 0.45081260800361633, 'learning_rate': 7.590803727231682e-05, 'epoch': 1.2204424103737606}\n",
      "‚è±Ô∏è Step 4900 - Elapsed: 44.06 min - Logs: {'loss': 0.5441, 'grad_norm': 0.518176257610321, 'learning_rate': 7.343304420833518e-05, 'epoch': 1.2458682939232137}\n",
      "‚è±Ô∏è Step 5000 - Elapsed: 44.92 min - Logs: {'loss': 0.7282, 'grad_norm': 0.08270502835512161, 'learning_rate': 7.095805114435354e-05, 'epoch': 1.2712941774726672}\n",
      "‚è±Ô∏è Step 5100 - Elapsed: 45.8 min - Logs: {'loss': 0.8769, 'grad_norm': 69.96025085449219, 'learning_rate': 6.848305808037191e-05, 'epoch': 1.2967200610221206}\n",
      "‚è±Ô∏è Step 5200 - Elapsed: 46.68 min - Logs: {'loss': 0.7498, 'grad_norm': 0.371468722820282, 'learning_rate': 6.600806501639027e-05, 'epoch': 1.3221459445715738}\n",
      "‚è±Ô∏è Step 5300 - Elapsed: 47.54 min - Logs: {'loss': 0.6266, 'grad_norm': 37.32763671875, 'learning_rate': 6.353307195240863e-05, 'epoch': 1.3475718281210272}\n",
      "‚è±Ô∏è Step 5400 - Elapsed: 48.41 min - Logs: {'loss': 0.6258, 'grad_norm': 0.23159325122833252, 'learning_rate': 6.1058078888427e-05, 'epoch': 1.3729977116704806}\n",
      "‚è±Ô∏è Step 5500 - Elapsed: 49.36 min - Logs: {'loss': 0.5777, 'grad_norm': 1.0972765684127808, 'learning_rate': 5.8583085824445354e-05, 'epoch': 1.398423595219934}\n",
      "‚è±Ô∏è Step 5600 - Elapsed: 50.26 min - Logs: {'loss': 0.7171, 'grad_norm': 0.2590182423591614, 'learning_rate': 5.610809276046372e-05, 'epoch': 1.4238494787693872}\n",
      "‚è±Ô∏è Step 5700 - Elapsed: 51.18 min - Logs: {'loss': 0.5475, 'grad_norm': 61.589263916015625, 'learning_rate': 5.363309969648209e-05, 'epoch': 1.4492753623188406}\n",
      "‚è±Ô∏è Step 5800 - Elapsed: 52.11 min - Logs: {'loss': 0.6583, 'grad_norm': 23.818809509277344, 'learning_rate': 5.1158106632500444e-05, 'epoch': 1.474701245868294}\n",
      "‚è±Ô∏è Step 5900 - Elapsed: 53.0 min - Logs: {'loss': 0.4705, 'grad_norm': 0.19369618594646454, 'learning_rate': 4.868311356851881e-05, 'epoch': 1.5001271294177472}\n",
      "‚è±Ô∏è Step 6000 - Elapsed: 53.85 min - Logs: {'loss': 0.7851, 'grad_norm': 15.193580627441406, 'learning_rate': 4.620812050453717e-05, 'epoch': 1.5255530129672006}\n",
      "‚è±Ô∏è Step 6100 - Elapsed: 54.69 min - Logs: {'loss': 0.632, 'grad_norm': 0.45358386635780334, 'learning_rate': 4.3733127440555534e-05, 'epoch': 1.550978896516654}\n",
      "‚è±Ô∏è Step 6200 - Elapsed: 55.55 min - Logs: {'loss': 0.5622, 'grad_norm': 87.40908813476562, 'learning_rate': 4.125813437657389e-05, 'epoch': 1.5764047800661074}\n",
      "‚è±Ô∏è Step 6300 - Elapsed: 56.4 min - Logs: {'loss': 0.9619, 'grad_norm': 4.114219665527344, 'learning_rate': 3.8783141312592254e-05, 'epoch': 1.6018306636155606}\n",
      "‚è±Ô∏è Step 6400 - Elapsed: 57.23 min - Logs: {'loss': 0.6549, 'grad_norm': 60.04426956176758, 'learning_rate': 3.6308148248610625e-05, 'epoch': 1.627256547165014}\n",
      "‚è±Ô∏è Step 6500 - Elapsed: 58.07 min - Logs: {'loss': 0.5313, 'grad_norm': 0.013756205327808857, 'learning_rate': 3.383315518462898e-05, 'epoch': 1.6526824307144672}\n",
      "‚è±Ô∏è Step 6600 - Elapsed: 58.91 min - Logs: {'loss': 0.8543, 'grad_norm': 79.45742797851562, 'learning_rate': 3.1358162120647345e-05, 'epoch': 1.6781083142639206}\n",
      "‚è±Ô∏è Step 6700 - Elapsed: 59.75 min - Logs: {'loss': 0.585, 'grad_norm': 4.106047630310059, 'learning_rate': 2.8883169056665708e-05, 'epoch': 1.703534197813374}\n",
      "‚è±Ô∏è Step 6800 - Elapsed: 60.6 min - Logs: {'loss': 0.471, 'grad_norm': 0.17039687931537628, 'learning_rate': 2.6408175992684068e-05, 'epoch': 1.7289600813628274}\n",
      "‚è±Ô∏è Step 6900 - Elapsed: 61.51 min - Logs: {'loss': 0.5814, 'grad_norm': 23.89667320251465, 'learning_rate': 2.3933182928702435e-05, 'epoch': 1.7543859649122808}\n",
      "‚è±Ô∏è Step 7000 - Elapsed: 62.39 min - Logs: {'loss': 0.5335, 'grad_norm': 1.4116661548614502, 'learning_rate': 2.1458189864720798e-05, 'epoch': 1.779811848461734}\n",
      "‚è±Ô∏è Step 7100 - Elapsed: 63.28 min - Logs: {'loss': 0.389, 'grad_norm': 12.621598243713379, 'learning_rate': 1.8983196800739158e-05, 'epoch': 1.8052377320111874}\n",
      "‚è±Ô∏è Step 7200 - Elapsed: 64.15 min - Logs: {'loss': 0.7982, 'grad_norm': 0.10409930348396301, 'learning_rate': 1.650820373675752e-05, 'epoch': 1.8306636155606406}\n",
      "‚è±Ô∏è Step 7300 - Elapsed: 65.0 min - Logs: {'loss': 0.6425, 'grad_norm': 0.2947806119918823, 'learning_rate': 1.4033210672775885e-05, 'epoch': 1.856089499110094}\n",
      "‚è±Ô∏è Step 7400 - Elapsed: 65.85 min - Logs: {'loss': 0.3224, 'grad_norm': 0.2617417573928833, 'learning_rate': 1.1558217608794247e-05, 'epoch': 1.8815153826595474}\n",
      "‚è±Ô∏è Step 7500 - Elapsed: 66.69 min - Logs: {'loss': 0.6631, 'grad_norm': 0.6728063225746155, 'learning_rate': 9.083224544812608e-06, 'epoch': 1.9069412662090008}\n",
      "‚è±Ô∏è Step 7600 - Elapsed: 67.52 min - Logs: {'loss': 0.644, 'grad_norm': 0.9717585444450378, 'learning_rate': 6.608231480830972e-06, 'epoch': 1.9323671497584543}\n",
      "‚è±Ô∏è Step 7700 - Elapsed: 68.39 min - Logs: {'loss': 0.7049, 'grad_norm': 64.89533996582031, 'learning_rate': 4.1332384168493344e-06, 'epoch': 1.9577930333079074}\n",
      "‚è±Ô∏è Step 7800 - Elapsed: 69.28 min - Logs: {'loss': 0.705, 'grad_norm': 0.007804150693118572, 'learning_rate': 1.658245352867697e-06, 'epoch': 1.9832189168573608}\n",
      "‚è±Ô∏è Step 7866 - Elapsed: 70.84 min - Logs: {'eval_loss': 0.7621033787727356, 'eval_f1': 0.8521103475139769, 'eval_runtime': 59.727, 'eval_samples_per_second': 16.475, 'eval_steps_per_second': 2.059, 'epoch': 2.0}\n",
      "‚è±Ô∏è Step 7866 - Elapsed: 70.84 min - Logs: {'train_runtime': 4250.6818, 'train_samples_per_second': 3.701, 'train_steps_per_second': 1.851, 'total_flos': 8834654115201024.0, 'train_loss': 0.7875856367528756, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 17:50:03,879] Trial 1 finished with value: 0.8521103475139769 and parameters: {'learning_rate': 0.0001946829544127956, 'per_device_train_batch_size': 2, 'num_train_epochs': 2}. Best is trial 1 with value: 0.8521103475139769.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at upstage/TinySolar-248m-4k-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting trial at 17:50:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11799' max='11799' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11799/11799 1:41:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.343400</td>\n",
       "      <td>0.951939</td>\n",
       "      <td>0.776521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.929300</td>\n",
       "      <td>1.023146</td>\n",
       "      <td>0.787800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>0.922836</td>\n",
       "      <td>0.807829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Step 100 - Elapsed: 0.84 min - Logs: {'loss': 1.2241, 'grad_norm': 27.575408935546875, 'learning_rate': 4.417847068451711e-05, 'epoch': 0.025425883549453344}\n",
      "‚è±Ô∏è Step 200 - Elapsed: 1.68 min - Logs: {'loss': 1.0907, 'grad_norm': 12.87486743927002, 'learning_rate': 4.3800876917982775e-05, 'epoch': 0.05085176709890669}\n",
      "‚è±Ô∏è Step 300 - Elapsed: 2.52 min - Logs: {'loss': 0.9225, 'grad_norm': 17.592485427856445, 'learning_rate': 4.3423283151448436e-05, 'epoch': 0.07627765064836003}\n",
      "‚è±Ô∏è Step 400 - Elapsed: 3.36 min - Logs: {'loss': 0.825, 'grad_norm': 26.757970809936523, 'learning_rate': 4.30456893849141e-05, 'epoch': 0.10170353419781338}\n",
      "‚è±Ô∏è Step 500 - Elapsed: 4.22 min - Logs: {'loss': 0.7758, 'grad_norm': 56.68157958984375, 'learning_rate': 4.266809561837977e-05, 'epoch': 0.12712941774726672}\n",
      "‚è±Ô∏è Step 600 - Elapsed: 5.04 min - Logs: {'loss': 0.8116, 'grad_norm': 21.6117000579834, 'learning_rate': 4.2290501851845437e-05, 'epoch': 0.15255530129672007}\n",
      "‚è±Ô∏è Step 700 - Elapsed: 5.88 min - Logs: {'loss': 0.7777, 'grad_norm': 67.28688049316406, 'learning_rate': 4.1912908085311104e-05, 'epoch': 0.1779811848461734}\n",
      "‚è±Ô∏è Step 800 - Elapsed: 6.82 min - Logs: {'loss': 0.8914, 'grad_norm': 2.7945141792297363, 'learning_rate': 4.153531431877677e-05, 'epoch': 0.20340706839562675}\n",
      "‚è±Ô∏è Step 900 - Elapsed: 7.77 min - Logs: {'loss': 0.7568, 'grad_norm': 53.01689529418945, 'learning_rate': 4.115772055224244e-05, 'epoch': 0.2288329519450801}\n",
      "‚è±Ô∏è Step 1000 - Elapsed: 8.64 min - Logs: {'loss': 0.8551, 'grad_norm': 4.528645992279053, 'learning_rate': 4.07801267857081e-05, 'epoch': 0.25425883549453343}\n",
      "‚è±Ô∏è Step 1100 - Elapsed: 9.51 min - Logs: {'loss': 0.7087, 'grad_norm': 127.8359603881836, 'learning_rate': 4.0402533019173765e-05, 'epoch': 0.2796847190439868}\n",
      "‚è±Ô∏è Step 1200 - Elapsed: 10.44 min - Logs: {'loss': 0.6714, 'grad_norm': 9.674560546875, 'learning_rate': 4.002493925263943e-05, 'epoch': 0.30511060259344014}\n",
      "‚è±Ô∏è Step 1300 - Elapsed: 11.38 min - Logs: {'loss': 0.9303, 'grad_norm': 121.91719055175781, 'learning_rate': 3.964734548610509e-05, 'epoch': 0.3305364861428935}\n",
      "‚è±Ô∏è Step 1400 - Elapsed: 12.36 min - Logs: {'loss': 0.9243, 'grad_norm': 3.905951499938965, 'learning_rate': 3.926975171957076e-05, 'epoch': 0.3559623696923468}\n",
      "‚è±Ô∏è Step 1500 - Elapsed: 13.37 min - Logs: {'loss': 0.98, 'grad_norm': 100.79769897460938, 'learning_rate': 3.8892157953036426e-05, 'epoch': 0.38138825324180015}\n",
      "‚è±Ô∏è Step 1600 - Elapsed: 14.3 min - Logs: {'loss': 1.0636, 'grad_norm': 5.384242534637451, 'learning_rate': 3.851456418650209e-05, 'epoch': 0.4068141367912535}\n",
      "‚è±Ô∏è Step 1700 - Elapsed: 15.22 min - Logs: {'loss': 0.8347, 'grad_norm': 0.12994854152202606, 'learning_rate': 3.813697041996776e-05, 'epoch': 0.43224002034070685}\n",
      "‚è±Ô∏è Step 1800 - Elapsed: 16.17 min - Logs: {'loss': 0.9384, 'grad_norm': 10.873756408691406, 'learning_rate': 3.775937665343342e-05, 'epoch': 0.4576659038901602}\n",
      "‚è±Ô∏è Step 1900 - Elapsed: 17.08 min - Logs: {'loss': 0.7413, 'grad_norm': 201.5869140625, 'learning_rate': 3.738178288689909e-05, 'epoch': 0.4830917874396135}\n",
      "‚è±Ô∏è Step 2000 - Elapsed: 17.99 min - Logs: {'loss': 0.8758, 'grad_norm': 127.35800170898438, 'learning_rate': 3.7004189120364754e-05, 'epoch': 0.5085176709890669}\n",
      "‚è±Ô∏è Step 2100 - Elapsed: 18.85 min - Logs: {'loss': 0.9957, 'grad_norm': 6.905462741851807, 'learning_rate': 3.662659535383042e-05, 'epoch': 0.5339435545385202}\n",
      "‚è±Ô∏è Step 2200 - Elapsed: 19.69 min - Logs: {'loss': 0.7667, 'grad_norm': 0.38587504625320435, 'learning_rate': 3.624900158729609e-05, 'epoch': 0.5593694380879736}\n",
      "‚è±Ô∏è Step 2300 - Elapsed: 20.52 min - Logs: {'loss': 1.0019, 'grad_norm': 133.69454956054688, 'learning_rate': 3.5871407820761755e-05, 'epoch': 0.5847953216374269}\n",
      "‚è±Ô∏è Step 2400 - Elapsed: 21.36 min - Logs: {'loss': 0.7875, 'grad_norm': 112.42913055419922, 'learning_rate': 3.549381405422742e-05, 'epoch': 0.6102212051868803}\n",
      "‚è±Ô∏è Step 2500 - Elapsed: 22.21 min - Logs: {'loss': 0.9739, 'grad_norm': 43.441680908203125, 'learning_rate': 3.511622028769309e-05, 'epoch': 0.6356470887363336}\n",
      "‚è±Ô∏è Step 2600 - Elapsed: 23.08 min - Logs: {'loss': 0.8996, 'grad_norm': 116.6132583618164, 'learning_rate': 3.473862652115875e-05, 'epoch': 0.661072972285787}\n",
      "‚è±Ô∏è Step 2700 - Elapsed: 23.96 min - Logs: {'loss': 0.8359, 'grad_norm': 58.33730697631836, 'learning_rate': 3.436103275462442e-05, 'epoch': 0.6864988558352403}\n",
      "‚è±Ô∏è Step 2800 - Elapsed: 24.77 min - Logs: {'loss': 1.0161, 'grad_norm': 56.670188903808594, 'learning_rate': 3.3983438988090084e-05, 'epoch': 0.7119247393846936}\n",
      "‚è±Ô∏è Step 2900 - Elapsed: 25.57 min - Logs: {'loss': 0.8794, 'grad_norm': 8.615917205810547, 'learning_rate': 3.360584522155575e-05, 'epoch': 0.737350622934147}\n",
      "‚è±Ô∏è Step 3000 - Elapsed: 26.39 min - Logs: {'loss': 1.0441, 'grad_norm': 124.36096954345703, 'learning_rate': 3.322825145502141e-05, 'epoch': 0.7627765064836003}\n",
      "‚è±Ô∏è Step 3100 - Elapsed: 27.22 min - Logs: {'loss': 0.7886, 'grad_norm': 1.5320404767990112, 'learning_rate': 3.285065768848708e-05, 'epoch': 0.7882023900330537}\n",
      "‚è±Ô∏è Step 3200 - Elapsed: 28.14 min - Logs: {'loss': 0.9897, 'grad_norm': 104.30758666992188, 'learning_rate': 3.2473063921952745e-05, 'epoch': 0.813628273582507}\n",
      "‚è±Ô∏è Step 3300 - Elapsed: 29.03 min - Logs: {'loss': 1.1874, 'grad_norm': 90.95929718017578, 'learning_rate': 3.2095470155418405e-05, 'epoch': 0.8390541571319603}\n",
      "‚è±Ô∏è Step 3400 - Elapsed: 29.92 min - Logs: {'loss': 1.2254, 'grad_norm': 105.8824234008789, 'learning_rate': 3.171787638888407e-05, 'epoch': 0.8644800406814137}\n",
      "‚è±Ô∏è Step 3500 - Elapsed: 30.8 min - Logs: {'loss': 0.6448, 'grad_norm': 79.9947509765625, 'learning_rate': 3.134028262234974e-05, 'epoch': 0.889905924230867}\n",
      "‚è±Ô∏è Step 3600 - Elapsed: 31.7 min - Logs: {'loss': 1.042, 'grad_norm': 59.832489013671875, 'learning_rate': 3.0962688855815406e-05, 'epoch': 0.9153318077803204}\n",
      "‚è±Ô∏è Step 3700 - Elapsed: 32.59 min - Logs: {'loss': 0.9634, 'grad_norm': 170.7144317626953, 'learning_rate': 3.0585095089281073e-05, 'epoch': 0.9407576913297737}\n",
      "‚è±Ô∏è Step 3800 - Elapsed: 33.46 min - Logs: {'loss': 1.0728, 'grad_norm': 2.679368019104004, 'learning_rate': 3.020750132274674e-05, 'epoch': 0.966183574879227}\n",
      "‚è±Ô∏è Step 3900 - Elapsed: 34.37 min - Logs: {'loss': 1.3434, 'grad_norm': 67.66887664794922, 'learning_rate': 2.9829907556212407e-05, 'epoch': 0.9916094584286804}\n",
      "‚è±Ô∏è Step 3933 - Elapsed: 35.69 min - Logs: {'eval_loss': 0.9519389271736145, 'eval_f1': 0.7765207165034842, 'eval_runtime': 60.8004, 'eval_samples_per_second': 16.184, 'eval_steps_per_second': 2.023, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Step 4000 - Elapsed: 36.29 min - Logs: {'loss': 0.735, 'grad_norm': 0.00980284158140421, 'learning_rate': 2.9452313789678074e-05, 'epoch': 1.0170353419781337}\n",
      "‚è±Ô∏è Step 4100 - Elapsed: 37.15 min - Logs: {'loss': 0.6462, 'grad_norm': 0.4147035479545593, 'learning_rate': 2.9074720023143735e-05, 'epoch': 1.0424612255275871}\n",
      "‚è±Ô∏è Step 4200 - Elapsed: 38.05 min - Logs: {'loss': 0.9225, 'grad_norm': 135.80538940429688, 'learning_rate': 2.86971262566094e-05, 'epoch': 1.0678871090770405}\n",
      "‚è±Ô∏è Step 4300 - Elapsed: 38.93 min - Logs: {'loss': 0.9229, 'grad_norm': 0.3170783817768097, 'learning_rate': 2.831953249007507e-05, 'epoch': 1.0933129926264937}\n",
      "‚è±Ô∏è Step 4400 - Elapsed: 39.79 min - Logs: {'loss': 0.6156, 'grad_norm': 0.3099701404571533, 'learning_rate': 2.7941938723540736e-05, 'epoch': 1.1187388761759471}\n",
      "‚è±Ô∏è Step 4500 - Elapsed: 40.62 min - Logs: {'loss': 1.1027, 'grad_norm': 124.02334594726562, 'learning_rate': 2.75643449570064e-05, 'epoch': 1.1441647597254005}\n",
      "‚è±Ô∏è Step 4600 - Elapsed: 41.45 min - Logs: {'loss': 0.9193, 'grad_norm': 17.903234481811523, 'learning_rate': 2.7186751190472066e-05, 'epoch': 1.1695906432748537}\n",
      "‚è±Ô∏è Step 4700 - Elapsed: 42.27 min - Logs: {'loss': 1.0335, 'grad_norm': 0.10934167355298996, 'learning_rate': 2.6809157423937733e-05, 'epoch': 1.1950165268243071}\n",
      "‚è±Ô∏è Step 4800 - Elapsed: 43.1 min - Logs: {'loss': 0.9606, 'grad_norm': 1.2831268310546875, 'learning_rate': 2.6431563657403394e-05, 'epoch': 1.2204424103737606}\n",
      "‚è±Ô∏è Step 4900 - Elapsed: 43.91 min - Logs: {'loss': 0.782, 'grad_norm': 6.4942402839660645, 'learning_rate': 2.605396989086906e-05, 'epoch': 1.2458682939232137}\n",
      "‚è±Ô∏è Step 5000 - Elapsed: 44.72 min - Logs: {'loss': 0.9678, 'grad_norm': 1.1473276615142822, 'learning_rate': 2.5676376124334728e-05, 'epoch': 1.2712941774726672}\n",
      "‚è±Ô∏è Step 5100 - Elapsed: 45.55 min - Logs: {'loss': 1.0523, 'grad_norm': 105.03343963623047, 'learning_rate': 2.5298782357800395e-05, 'epoch': 1.2967200610221206}\n",
      "‚è±Ô∏è Step 5200 - Elapsed: 46.36 min - Logs: {'loss': 0.9694, 'grad_norm': 2.1283562183380127, 'learning_rate': 2.492118859126606e-05, 'epoch': 1.3221459445715738}\n",
      "‚è±Ô∏è Step 5300 - Elapsed: 47.16 min - Logs: {'loss': 0.6503, 'grad_norm': 20.860502243041992, 'learning_rate': 2.454359482473173e-05, 'epoch': 1.3475718281210272}\n",
      "‚è±Ô∏è Step 5400 - Elapsed: 47.98 min - Logs: {'loss': 0.7619, 'grad_norm': 0.7347568869590759, 'learning_rate': 2.4166001058197392e-05, 'epoch': 1.3729977116704806}\n",
      "‚è±Ô∏è Step 5500 - Elapsed: 48.78 min - Logs: {'loss': 0.7843, 'grad_norm': 5.857999801635742, 'learning_rate': 2.3788407291663056e-05, 'epoch': 1.398423595219934}\n",
      "‚è±Ô∏è Step 5600 - Elapsed: 49.58 min - Logs: {'loss': 0.8745, 'grad_norm': 1.7798255681991577, 'learning_rate': 2.3410813525128723e-05, 'epoch': 1.4238494787693872}\n",
      "‚è±Ô∏è Step 5700 - Elapsed: 50.38 min - Logs: {'loss': 0.7578, 'grad_norm': 141.63575744628906, 'learning_rate': 2.3033219758594387e-05, 'epoch': 1.4492753623188406}\n",
      "‚è±Ô∏è Step 5800 - Elapsed: 51.18 min - Logs: {'loss': 0.8652, 'grad_norm': 7.0559821128845215, 'learning_rate': 2.2655625992060054e-05, 'epoch': 1.474701245868294}\n",
      "‚è±Ô∏è Step 5900 - Elapsed: 51.98 min - Logs: {'loss': 0.6433, 'grad_norm': 0.4003537595272064, 'learning_rate': 2.227803222552572e-05, 'epoch': 1.5001271294177472}\n",
      "‚è±Ô∏è Step 6000 - Elapsed: 52.78 min - Logs: {'loss': 0.8882, 'grad_norm': 4.619743824005127, 'learning_rate': 2.1900438458991388e-05, 'epoch': 1.5255530129672006}\n",
      "‚è±Ô∏è Step 6100 - Elapsed: 53.59 min - Logs: {'loss': 0.9336, 'grad_norm': 57.342041015625, 'learning_rate': 2.152284469245705e-05, 'epoch': 1.550978896516654}\n",
      "‚è±Ô∏è Step 6200 - Elapsed: 54.39 min - Logs: {'loss': 0.7314, 'grad_norm': 82.16012573242188, 'learning_rate': 2.1145250925922718e-05, 'epoch': 1.5764047800661074}\n",
      "‚è±Ô∏è Step 6300 - Elapsed: 55.19 min - Logs: {'loss': 1.2668, 'grad_norm': 44.125911712646484, 'learning_rate': 2.0767657159388385e-05, 'epoch': 1.6018306636155606}\n",
      "‚è±Ô∏è Step 6400 - Elapsed: 55.99 min - Logs: {'loss': 0.9944, 'grad_norm': 99.22662353515625, 'learning_rate': 2.039006339285405e-05, 'epoch': 1.627256547165014}\n",
      "‚è±Ô∏è Step 6500 - Elapsed: 56.79 min - Logs: {'loss': 0.717, 'grad_norm': 0.4407975971698761, 'learning_rate': 2.0012469626319716e-05, 'epoch': 1.6526824307144672}\n",
      "‚è±Ô∏è Step 6600 - Elapsed: 57.59 min - Logs: {'loss': 1.159, 'grad_norm': 143.96142578125, 'learning_rate': 1.963487585978538e-05, 'epoch': 1.6781083142639206}\n",
      "‚è±Ô∏è Step 6700 - Elapsed: 58.39 min - Logs: {'loss': 0.8442, 'grad_norm': 137.06884765625, 'learning_rate': 1.9257282093251047e-05, 'epoch': 1.703534197813374}\n",
      "‚è±Ô∏è Step 6800 - Elapsed: 59.2 min - Logs: {'loss': 0.7124, 'grad_norm': 0.6449376344680786, 'learning_rate': 1.887968832671671e-05, 'epoch': 1.7289600813628274}\n",
      "‚è±Ô∏è Step 6900 - Elapsed: 60.0 min - Logs: {'loss': 0.9303, 'grad_norm': 139.0689239501953, 'learning_rate': 1.8502094560182377e-05, 'epoch': 1.7543859649122808}\n",
      "‚è±Ô∏è Step 7000 - Elapsed: 60.8 min - Logs: {'loss': 0.8479, 'grad_norm': 0.2931997776031494, 'learning_rate': 1.8124500793648044e-05, 'epoch': 1.779811848461734}\n",
      "‚è±Ô∏è Step 7100 - Elapsed: 61.6 min - Logs: {'loss': 0.7154, 'grad_norm': 0.9448803663253784, 'learning_rate': 1.774690702711371e-05, 'epoch': 1.8052377320111874}\n",
      "‚è±Ô∏è Step 7200 - Elapsed: 62.4 min - Logs: {'loss': 0.9904, 'grad_norm': 0.17571601271629333, 'learning_rate': 1.7369313260579375e-05, 'epoch': 1.8306636155606406}\n",
      "‚è±Ô∏è Step 7300 - Elapsed: 63.2 min - Logs: {'loss': 0.7278, 'grad_norm': 14.412786483764648, 'learning_rate': 1.6991719494045042e-05, 'epoch': 1.856089499110094}\n",
      "‚è±Ô∏è Step 7400 - Elapsed: 64.01 min - Logs: {'loss': 0.6385, 'grad_norm': 0.06574606150388718, 'learning_rate': 1.6614125727510706e-05, 'epoch': 1.8815153826595474}\n",
      "‚è±Ô∏è Step 7500 - Elapsed: 64.81 min - Logs: {'loss': 0.8847, 'grad_norm': 0.18418549001216888, 'learning_rate': 1.6236531960976373e-05, 'epoch': 1.9069412662090008}\n",
      "‚è±Ô∏è Step 7600 - Elapsed: 65.61 min - Logs: {'loss': 0.8983, 'grad_norm': 14.121166229248047, 'learning_rate': 1.5858938194442036e-05, 'epoch': 1.9323671497584543}\n",
      "‚è±Ô∏è Step 7700 - Elapsed: 66.42 min - Logs: {'loss': 0.827, 'grad_norm': 80.22978973388672, 'learning_rate': 1.5481344427907703e-05, 'epoch': 1.9577930333079074}\n",
      "‚è±Ô∏è Step 7800 - Elapsed: 67.22 min - Logs: {'loss': 0.9293, 'grad_norm': 0.02232903055846691, 'learning_rate': 1.510375066137337e-05, 'epoch': 1.9832189168573608}\n",
      "‚è±Ô∏è Step 7866 - Elapsed: 68.67 min - Logs: {'eval_loss': 1.0231457948684692, 'eval_f1': 0.7877998856966867, 'eval_runtime': 55.5454, 'eval_samples_per_second': 17.715, 'eval_steps_per_second': 2.214, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Step 7900 - Elapsed: 68.95 min - Logs: {'loss': 0.7012, 'grad_norm': 1.728736162185669, 'learning_rate': 1.4726156894839037e-05, 'epoch': 2.008644800406814}\n",
      "‚è±Ô∏è Step 8000 - Elapsed: 69.75 min - Logs: {'loss': 0.764, 'grad_norm': 110.74766540527344, 'learning_rate': 1.43485631283047e-05, 'epoch': 2.0340706839562674}\n",
      "‚è±Ô∏è Step 8100 - Elapsed: 70.55 min - Logs: {'loss': 0.836, 'grad_norm': 1.1847401857376099, 'learning_rate': 1.3970969361770368e-05, 'epoch': 2.059496567505721}\n",
      "‚è±Ô∏è Step 8200 - Elapsed: 71.36 min - Logs: {'loss': 0.797, 'grad_norm': 0.03496965393424034, 'learning_rate': 1.3593375595236033e-05, 'epoch': 2.0849224510551743}\n",
      "‚è±Ô∏è Step 8300 - Elapsed: 72.16 min - Logs: {'loss': 1.0405, 'grad_norm': 0.035080309957265854, 'learning_rate': 1.3215781828701697e-05, 'epoch': 2.1103483346046277}\n",
      "‚è±Ô∏è Step 8400 - Elapsed: 72.96 min - Logs: {'loss': 0.6654, 'grad_norm': 24.67559051513672, 'learning_rate': 1.2838188062167364e-05, 'epoch': 2.135774218154081}\n",
      "‚è±Ô∏è Step 8500 - Elapsed: 73.76 min - Logs: {'loss': 0.7957, 'grad_norm': 0.18005719780921936, 'learning_rate': 1.246059429563303e-05, 'epoch': 2.161200101703534}\n",
      "‚è±Ô∏è Step 8600 - Elapsed: 74.56 min - Logs: {'loss': 0.7228, 'grad_norm': 0.423602432012558, 'learning_rate': 1.2083000529098696e-05, 'epoch': 2.1866259852529875}\n",
      "‚è±Ô∏è Step 8700 - Elapsed: 75.36 min - Logs: {'loss': 0.6349, 'grad_norm': 124.99435424804688, 'learning_rate': 1.1705406762564361e-05, 'epoch': 2.212051868802441}\n",
      "‚è±Ô∏è Step 8800 - Elapsed: 76.17 min - Logs: {'loss': 0.7604, 'grad_norm': 163.7718048095703, 'learning_rate': 1.1327812996030027e-05, 'epoch': 2.2374777523518943}\n",
      "‚è±Ô∏è Step 8900 - Elapsed: 76.97 min - Logs: {'loss': 0.5593, 'grad_norm': 37.01141357421875, 'learning_rate': 1.0950219229495694e-05, 'epoch': 2.2629036359013477}\n",
      "‚è±Ô∏è Step 9000 - Elapsed: 77.78 min - Logs: {'loss': 0.7744, 'grad_norm': 26.895517349243164, 'learning_rate': 1.0572625462961359e-05, 'epoch': 2.288329519450801}\n",
      "‚è±Ô∏è Step 9100 - Elapsed: 78.58 min - Logs: {'loss': 0.8289, 'grad_norm': 0.300289511680603, 'learning_rate': 1.0195031696427024e-05, 'epoch': 2.313755403000254}\n",
      "‚è±Ô∏è Step 9200 - Elapsed: 79.38 min - Logs: {'loss': 1.0024, 'grad_norm': 0.21884801983833313, 'learning_rate': 9.81743792989269e-06, 'epoch': 2.3391812865497075}\n",
      "‚è±Ô∏è Step 9300 - Elapsed: 80.19 min - Logs: {'loss': 0.7569, 'grad_norm': 63.45752716064453, 'learning_rate': 9.439844163358355e-06, 'epoch': 2.364607170099161}\n",
      "‚è±Ô∏è Step 9400 - Elapsed: 80.99 min - Logs: {'loss': 0.5626, 'grad_norm': 0.6820323467254639, 'learning_rate': 9.062250396824022e-06, 'epoch': 2.3900330536486143}\n",
      "‚è±Ô∏è Step 9500 - Elapsed: 81.79 min - Logs: {'loss': 0.6999, 'grad_norm': 0.0919145718216896, 'learning_rate': 8.684656630289687e-06, 'epoch': 2.4154589371980677}\n",
      "‚è±Ô∏è Step 9600 - Elapsed: 82.59 min - Logs: {'loss': 0.7036, 'grad_norm': 1.212795376777649, 'learning_rate': 8.307062863755353e-06, 'epoch': 2.440884820747521}\n",
      "‚è±Ô∏è Step 9700 - Elapsed: 83.39 min - Logs: {'loss': 0.9164, 'grad_norm': 30.832395553588867, 'learning_rate': 7.929469097221018e-06, 'epoch': 2.4663107042969745}\n",
      "‚è±Ô∏è Step 9800 - Elapsed: 84.2 min - Logs: {'loss': 0.824, 'grad_norm': 252.20205688476562, 'learning_rate': 7.551875330686685e-06, 'epoch': 2.4917365878464275}\n",
      "‚è±Ô∏è Step 9900 - Elapsed: 85.0 min - Logs: {'loss': 0.9208, 'grad_norm': 168.8931121826172, 'learning_rate': 7.17428156415235e-06, 'epoch': 2.517162471395881}\n",
      "‚è±Ô∏è Step 10000 - Elapsed: 85.8 min - Logs: {'loss': 0.7741, 'grad_norm': 0.15949800610542297, 'learning_rate': 6.796687797618017e-06, 'epoch': 2.5425883549453343}\n",
      "‚è±Ô∏è Step 10100 - Elapsed: 86.6 min - Logs: {'loss': 0.6898, 'grad_norm': 0.13355647027492523, 'learning_rate': 6.419094031083682e-06, 'epoch': 2.5680142384947877}\n",
      "‚è±Ô∏è Step 10200 - Elapsed: 87.4 min - Logs: {'loss': 0.8151, 'grad_norm': 86.50643157958984, 'learning_rate': 6.041500264549348e-06, 'epoch': 2.593440122044241}\n",
      "‚è±Ô∏è Step 10300 - Elapsed: 88.2 min - Logs: {'loss': 0.7928, 'grad_norm': 10.13792896270752, 'learning_rate': 5.663906498015013e-06, 'epoch': 2.6188660055936945}\n",
      "‚è±Ô∏è Step 10400 - Elapsed: 89.0 min - Logs: {'loss': 0.7973, 'grad_norm': 24.796459197998047, 'learning_rate': 5.2863127314806796e-06, 'epoch': 2.6442918891431475}\n",
      "‚è±Ô∏è Step 10500 - Elapsed: 89.81 min - Logs: {'loss': 0.7744, 'grad_norm': 0.9428504109382629, 'learning_rate': 4.908718964946345e-06, 'epoch': 2.669717772692601}\n",
      "‚è±Ô∏è Step 10600 - Elapsed: 90.61 min - Logs: {'loss': 0.8684, 'grad_norm': 0.2108699530363083, 'learning_rate': 4.531125198412011e-06, 'epoch': 2.6951436562420543}\n",
      "‚è±Ô∏è Step 10700 - Elapsed: 91.41 min - Logs: {'loss': 0.6862, 'grad_norm': 0.25144433975219727, 'learning_rate': 4.153531431877676e-06, 'epoch': 2.7205695397915077}\n",
      "‚è±Ô∏è Step 10800 - Elapsed: 92.21 min - Logs: {'loss': 0.4982, 'grad_norm': 0.5750502943992615, 'learning_rate': 3.7759376653433426e-06, 'epoch': 2.745995423340961}\n",
      "‚è±Ô∏è Step 10900 - Elapsed: 93.01 min - Logs: {'loss': 0.7511, 'grad_norm': 252.96307373046875, 'learning_rate': 3.3983438988090083e-06, 'epoch': 2.7714213068904145}\n",
      "‚è±Ô∏è Step 11000 - Elapsed: 93.81 min - Logs: {'loss': 0.7411, 'grad_norm': 0.7020334005355835, 'learning_rate': 3.020750132274674e-06, 'epoch': 2.796847190439868}\n",
      "‚è±Ô∏è Step 11100 - Elapsed: 94.61 min - Logs: {'loss': 0.7864, 'grad_norm': 0.6631749272346497, 'learning_rate': 2.6431563657403398e-06, 'epoch': 2.8222730739893214}\n",
      "‚è±Ô∏è Step 11200 - Elapsed: 95.42 min - Logs: {'loss': 0.8153, 'grad_norm': 0.07592872530221939, 'learning_rate': 2.2655625992060055e-06, 'epoch': 2.8476989575387743}\n",
      "‚è±Ô∏è Step 11300 - Elapsed: 96.22 min - Logs: {'loss': 0.5614, 'grad_norm': 5.248464107513428, 'learning_rate': 1.8879688326716713e-06, 'epoch': 2.8731248410882277}\n",
      "‚è±Ô∏è Step 11400 - Elapsed: 97.02 min - Logs: {'loss': 0.6594, 'grad_norm': 8.467819213867188, 'learning_rate': 1.510375066137337e-06, 'epoch': 2.898550724637681}\n",
      "‚è±Ô∏è Step 11500 - Elapsed: 97.83 min - Logs: {'loss': 0.6442, 'grad_norm': 0.06955355405807495, 'learning_rate': 1.1327812996030028e-06, 'epoch': 2.9239766081871346}\n",
      "‚è±Ô∏è Step 11600 - Elapsed: 98.63 min - Logs: {'loss': 0.6861, 'grad_norm': 1.2330126762390137, 'learning_rate': 7.551875330686685e-07, 'epoch': 2.949402491736588}\n",
      "‚è±Ô∏è Step 11700 - Elapsed: 99.43 min - Logs: {'loss': 0.8544, 'grad_norm': 0.03610973432660103, 'learning_rate': 3.7759376653433426e-07, 'epoch': 2.974828375286041}\n",
      "‚è±Ô∏è Step 11799 - Elapsed: 101.32 min - Logs: {'eval_loss': 0.922836184501648, 'eval_f1': 0.8078289267912389, 'eval_runtime': 59.9831, 'eval_samples_per_second': 16.405, 'eval_steps_per_second': 2.051, 'epoch': 3.0}\n",
      "‚è±Ô∏è Step 11799 - Elapsed: 101.32 min - Logs: {'train_runtime': 6079.4931, 'train_samples_per_second': 3.882, 'train_steps_per_second': 1.941, 'total_flos': 1.3251981172801536e+16, 'train_loss': 0.8456051779031774, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 19:31:25,071] Trial 2 finished with value: 0.8078289267912389 and parameters: {'learning_rate': 4.45522885133861e-05, 'per_device_train_batch_size': 2, 'num_train_epochs': 3}. Best is trial 1 with value: 0.8521103475139769.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done!\n",
      "‚è±Ô∏è Optuna tuning done in 253.94 minutes\n",
      "üèÜ Best trial params: {'learning_rate': 0.0001946829544127956, 'per_device_train_batch_size': 2, 'num_train_epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import optuna\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "#  Class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(df[\"label\"]),\n",
    "    y=df[\"label\"]\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "\n",
    "#  TRACK TIME PER TRIAL\n",
    "class TrialProgressCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nüöÄ Starting trial at {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"‚è±Ô∏è Step {state.global_step} - Elapsed: {round(elapsed/60, 2)} min - Logs: {logs}\")\n",
    "\n",
    "#  METRICS\n",
    "def compute_metrics(pred):\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\"f1\": f1_score(pred.label_ids, preds, average=\"weighted\")}\n",
    "\n",
    "#  MODEL INIT (with LoRA)\n",
    "def model_init():\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        num_labels=3,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS\n",
    "    )\n",
    "    lora_model = get_peft_model(base_model, lora_config)\n",
    "    return lora_model\n",
    "\n",
    "#  OPTUNA SEARCH SPACE\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 5e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 3)\n",
    "    }\n",
    "\n",
    "#  Custom Trainer with class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "#  TRAINING ARGS\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./optuna_output\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "#  TRAINER with everything\n",
    "trainer = WeightedTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        TrialProgressCallback(),\n",
    "        EarlyStoppingCallback(early_stopping_patience=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#  RUN OPTUNA\n",
    "start = time.time()\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=3\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\n‚úÖ Done!\")\n",
    "print(f\"‚è±Ô∏è Optuna tuning done in {round((end - start)/60, 2)} minutes\")\n",
    "print(\"üèÜ Best trial params:\", best_trial.hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Best trial F1 score: 0.8521103475139769\n",
      "üìã Best trial hyperparameters: {'learning_rate': 0.0001946829544127956, 'per_device_train_batch_size': 2, 'num_train_epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"üèÜ Best trial F1 score:\", best_trial.objective)\n",
    "print(\"üìã Best trial hyperparameters:\", best_trial.hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best hyperparameters saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Save best params\n",
    "with open(\"best_params_SOLAR.json\", \"w\") as f:\n",
    "    json.dump(best_trial.hyperparameters, f)\n",
    "\n",
    "print(\"‚úÖ Best hyperparameters saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Train and Validation Sets\n",
    "\n",
    "The training and validation datasets are merged into a single full training set.\n",
    "This allows the final model to be trained using all available labeled data for better performance, instead of wasting examples on separate validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and validation splits\n",
    "full_train_dataset = Dataset.from_dict({\n",
    "    key: tokenized_dataset[\"train\"][key] + tokenized_dataset[\"validation\"][key]\n",
    "    for key in tokenized_dataset[\"train\"].features\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final version, the model is fine-tuned using the best hyperparameters previously found with Optuna.\n",
    "\n",
    "Class imbalance is addressed by assigning a higher weight to the Neutral class during training.\n",
    "\n",
    "A custom WeightedTrainer is used to apply class weights correctly through a modified loss function (CrossEntropyLoss).\n",
    "\n",
    "The model is initialized with LoRA (Low-Rank Adaptation) on top of the model, using 4-bit quantization to optimize memory usage.\n",
    "\n",
    "Training is done on the full training dataset, with no evaluation during training (evaluation_strategy=\"no\"), to prevent data leakage.\n",
    "\n",
    "After training, predictions are made on the separate test set, and a threshold optimization is applied to better distinguish Neutral class predictions.\n",
    "\n",
    "Finally, classification metrics, a confusion matrix, and the weighted F1-score are printed to summarize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alicia\\AppData\\Roaming\\Python\\Python39\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alicia\\AppData\\Local\\Temp\\ipykernel_18800\\3141789759.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  final_trainer = WeightedTrainer(\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at upstage/TinySolar-248m-4k-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at upstage/TinySolar-248m-4k-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\Alicia\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13275' max='13275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13275/13275 1:56:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.910800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.871000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.625200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.609300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.285100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final training completed in 116.28 minutes.\n",
      " Final model saved to 'finalBALANCE' folder.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Best Neutral Threshold found: 0.394\n",
      "\n",
      "üìò Classification Report (Final):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.85      0.83       302\n",
      "    Positive       0.86      0.89      0.87       341\n",
      "     Neutral       0.49      0.40      0.44       167\n",
      "\n",
      "    accuracy                           0.78       810\n",
      "   macro avg       0.72      0.71      0.71       810\n",
      "weighted avg       0.76      0.78      0.77       810\n",
      "\n",
      "\n",
      "‚úÖ Confusion Matrix:\n",
      "[[258   4  40]\n",
      " [  8 303  30]\n",
      " [ 55  45  67]]\n",
      "\n",
      "üî¥ Final Weighted F1 (with threshold): 0.7678\n"
     ]
    }
   ],
   "source": [
    "#  Imports\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from torch import nn\n",
    "\n",
    "#  Load Best Parameters\n",
    "with open(\"best_params_SOLAR.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "#  Class weights\n",
    "class_weights = torch.tensor([1.0, 1.0, 2.0], dtype=torch.float).to(\"cuda\")  # Neutral weighted higher\n",
    "\n",
    "\n",
    "#  Custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "#  Model Init function\n",
    "def model_init():\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        num_labels=3,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    return model\n",
    "\n",
    "#  Training Args\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=\"./final\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",  \n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\",\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "#  Final Trainer\n",
    "final_trainer = WeightedTrainer(\n",
    "    model_init=model_init,\n",
    "    args=final_training_args,\n",
    "    train_dataset=full_train_dataset,  # <<< Full train\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#  Train\n",
    "start = time.time()\n",
    "final_trainer.train()\n",
    "end = time.time()\n",
    "print(f\"\\n‚úÖ Final training completed in {round((end-start)/60, 2)} minutes.\")\n",
    "\n",
    "#  Save Model\n",
    "final_trainer.save_model(\"finalBALANCE\")\n",
    "print(\" Final model saved to 'finalBALANCE' folder.\")\n",
    "\n",
    "#  Predictions\n",
    "predictions = final_trainer.predict(tokenized_dataset[\"test\"])\n",
    "probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "labels = predictions.label_ids\n",
    "\n",
    "#  Auto Threshold Optimization\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def threshold_objective(thresh):\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    max_probs = np.max(probs, axis=1)\n",
    "    preds[max_probs < thresh] = 2  # Force to Neutral if uncertain\n",
    "    return -f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "opt_result = minimize_scalar(threshold_objective, bounds=(0.3, 0.7), method=\"bounded\")\n",
    "best_thresh = opt_result.x\n",
    "print(f\"\\nüîç Best Neutral Threshold found: {round(best_thresh, 3)}\")\n",
    "\n",
    "#  Apply Best Threshold\n",
    "preds = np.argmax(probs, axis=1)\n",
    "max_probs = np.max(probs, axis=1)\n",
    "preds[max_probs < best_thresh] = 2  # Again force Neutral\n",
    "\n",
    "#  Print Metrics\n",
    "print(\"\\nüìò Classification Report (Final):\")\n",
    "print(classification_report(labels, preds, target_names=['Negative', 'Positive', 'Neutral']))\n",
    "\n",
    "print(\"\\n‚úÖ Confusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))\n",
    "\n",
    "final_f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "print(f\"\\nüî¥ Final Weighted F1 (with threshold): {round(final_f1, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Best Neutral Confidence Margin found: 0.157\n",
      "\n",
      "üìò Classification Report (Confidence + Temperature):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.81      0.84      0.83       302\n",
      "    Positive       0.87      0.88      0.87       341\n",
      "     Neutral       0.49      0.45      0.47       167\n",
      "\n",
      "    accuracy                           0.78       810\n",
      "   macro avg       0.73      0.72      0.72       810\n",
      "weighted avg       0.77      0.78      0.77       810\n",
      "\n",
      "\n",
      "‚úÖ Confusion Matrix:\n",
      "[[255   3  44]\n",
      " [  8 300  33]\n",
      " [ 50  42  75]]\n",
      "\n",
      "üî¥ Final Weighted F1 (with threshold): 0.7743\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# === Predict on test set ===\n",
    "predictions = final_trainer.predict(tokenized_dataset[\"test\"])\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# === Apply softmax with temperature scaling ===\n",
    "temperature = 2  # Try 1.5‚Äì2.0 if needed\n",
    "probs = F.softmax(torch.tensor(logits / temperature), dim=-1).numpy()\n",
    "\n",
    "# === Define threshold optimization using confidence margin ===\n",
    "def threshold_objective(margin):\n",
    "    final_preds = []\n",
    "    for p in probs:\n",
    "        top2 = np.sort(p)[-2:]        # take two highest probs\n",
    "        gap = top2[1] - top2[0]       # confidence gap\n",
    "        if gap < margin:\n",
    "            final_preds.append(2)     # force Neutral\n",
    "        else:\n",
    "            final_preds.append(np.argmax(p))\n",
    "    return -f1_score(labels, final_preds, average=\"weighted\")\n",
    "\n",
    "# === Find best neutral margin ===\n",
    "opt_result = minimize_scalar(threshold_objective, bounds=(0.05, 0.4), method=\"bounded\")\n",
    "neutral_margin = opt_result.x\n",
    "print(f\"\\nüîç Best Neutral Confidence Margin found: {round(neutral_margin, 3)}\")\n",
    "\n",
    "# === Apply margin to get final predictions ===\n",
    "final_preds = []\n",
    "for p in probs:\n",
    "    top2 = np.sort(p)[-2:]\n",
    "    gap = top2[1] - top2[0]\n",
    "    if gap < neutral_margin:\n",
    "        final_preds.append(2)\n",
    "    else:\n",
    "        final_preds.append(np.argmax(p))\n",
    "\n",
    "# === Evaluation ===\n",
    "print(\"\\nüìò Classification Report (Confidence + Temperature):\")\n",
    "print(classification_report(labels, final_preds, target_names=[\"Negative\", \"Positive\", \"Neutral\"]))\n",
    "\n",
    "print(\"\\n‚úÖ Confusion Matrix:\")\n",
    "print(confusion_matrix(labels, final_preds))\n",
    "\n",
    "final_f1 = f1_score(labels, final_preds, average=\"weighted\")\n",
    "print(f\"\\nüî¥ Final Weighted F1 (with threshold): {round(final_f1, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Accuracy: 0.7753\n",
      "‚úÖ Final Weighted F1-score: 0.7743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "final_accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "print(f\"\\n‚úÖ Final Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"‚úÖ Final Weighted F1-score: {final_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
